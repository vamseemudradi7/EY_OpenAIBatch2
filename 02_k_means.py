# -*- coding: utf-8 -*-
"""02 K-Means.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/a-forty-two/EY8Apr2024-AI-Batch2/blob/main/EY06_K_Means.ipynb
"""

# Previous-> SUPERVISED algos. Supervised algo means that you know what the label is
# LABEL was Diagnosis!
# when we don't know the label, then we can only CLASSIFY the points based on various factors,
# such as - are they kept next to each other?
# UNSUPERVISED ALGOS-> no idea what the label is, just figure out if things could be kept
# together



import pandas as pd
df = pd.DataFrame({
    'x':[12,20,28,18,29,33,24,45,52,45,51,52,55,53,55,61,65,66,72,22],
    'y':[39, 35, 30, 52, 55, 53, 46, 55, 59, 63,70, 66,63,58,23,14,8,19,7,24]
})
# THIS IS THE ONLY DATA THAT WE HAVE-> there is no output or label to guide us
# Hence, no xtrain,ytrain,xtest, ytest either!!!

"""1) ASSUME k number of CENTROIDS. These centroids are the no. of clusters you want to divide your data into

2) RANDOMLY select k points from given dataset or you can even select your OWN random points

3) CALCULATE DISTANCE of EVERY point from these centroids

4) EACH point will be classified to the closest (NEAREST) CENTROID

5) Is this the best answer? NO? [DISTANCE algorithms-> STD -> MIN STD for best algo]

6) MOVE THE CENTROIDS to their MEAN

7) GO TO STEP 3 again. Keep repeating till all the points are visited or min STD achieved.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
# if you want to use iPython's matplotlib
# inline -> use local definition not official python definition
# local definition-> Whatever Colab's iPython has decided
# here matplotlib is a HTML/CSS/JS library
# on local machine matplotlib is a C++ library

# to ensure we get the same randomness everytime we run random functions, we will fix the seed
# for randomness -> random_state
np.random.seed(42) # EVEN THO THERE IS RANDOMNESS -> SAME randomness is applied for all of us
k = 3 # ASSUMPTION
centroids = {i+1:[np.random.randint(0,80),np.random.randint(0,80) ]  for i in range(k)} # (x,y)
centroids

fig = plt.figure(figsize=(5,5))
plt.scatter(df['x'], df['y'], color='k') # k means black
# let's plot our centroids overlay on this scatter plot
color_dic = {1:'r', 2:'b', 3:'g'}
for i in centroids.keys():
  plt.scatter(*centroids[i], color=color_dic[i])
plt.xlim(0,80) # min and max of scale on x -axis
plt.ylim(0,80) # same as above on y axis
plt.show()

def Fit(df, centroids):
  for i in centroids.keys():
    # squared root distance formulae
    # np.root((x2-x1)**2 + (y2-y1)**2) -> (x2,y2) is the centroid, (x1,y1) are all my data points
    df['distance_from_{}'.format(i)] = (np.sqrt((df['x']-centroids[i][0])**2 + (df['y']-centroids[i][1])**2))
  # create new cols for comparison of which distance is least
  # CALCULATE DISTANCE from each centroid, and CREATE a COLUMN out of it in DF
  centroid_new_cols = ['distance_from_{}'.format(i) for i in centroids.keys()]
  # SELECT the column with SMALLEST DISTANCE -> IDXMIN --> Index of MINIMUM VALUE [ distance_from_1:10, distance_from_2:20, distance_from_3:15] -> distance_from_1
  df['closest'] = df.loc[:,centroid_new_cols].idxmin(axis=1) # axis=1, accessing a new dimension in existing datafram
  # distance_from_3 was smalled value, and that has been entered as CLOSEST value
  # df['closest'] -> distance_from_3, distance_from_1 and so on
  df['closest'] = df['closest'].map(lambda x: int(x.lstrip('distance_from_'))) # remove extra words and leave on centroid number
  # converting distance_from_3 to 3
  df['color'] = df['closest'].map(lambda x: color_dic[x])
  return df



df_modified = Fit(df, centroids)
df_modified.head(10)

fig = plt.figure(figsize=(5,5))
plt.scatter(df_modified['x'], df_modified['y'], color=df_modified['color']) # k means black
color_dic = {1:'r', 2:'b', 3:'g'}
for i in centroids.keys():
  plt.scatter(*centroids[i], color=color_dic[i]) # overlaying centroids
plt.xlim(0,80) # min and max of scale on x -axis
plt.ylim(0,80) # same as above on y axis
plt.show()
# EITHER syntax or SCIENCE

# update centroids
# prev centroids to move to new centroids
# DEEP copy of existing centroids
# https://www.geeksforgeeks.org/copy-python-deep-copy-shallow-copy/

import copy
old_centroids = copy.deepcopy(centroids)
def update_centroids(df, k):
  centroids_new = centroids
  for i in centroids.keys():
    centroids_new[i][0] = np.mean(df[df['closest']==i]['x']) # UPDATING to MEAN of the cluster (new centroid), instead of mean, if you took MODE, the same algo is called k-mode
    centroids_new[i][1] = np.mean(df[df['closest']==i]['y'])
  return centroids_new

new_centroids = update_centroids(df_modified, k )
new_centroids

df_new = Fit(df_modified, new_centroids)
fig = plt.figure(figsize=(5,5))
plt.scatter(df_new['x'], df_new['y'], color=df_new['color']) # k means black
color_dic2 = {1:'k', 2:'k', 3:'k'}
for i in new_centroids.keys():
  plt.scatter(*new_centroids[i], color=color_dic2[i]) # overlaying centroids
plt.xlim(0,80) # min and max of scale on x -axis
plt.ylim(0,80) # same as above on y axis
plt.show()

# KEEP UPDATING

while True:
  closest_centroids = df_new['closest'].copy(deep=True)  # DEEP COPY from DataFrame
  centroids = update_centroids(df_new, centroids)
  df_new = Fit(df_new, centroids)
  if closest_centroids.equals(df['closest']): # all nodes have optimized now, nothing further to traverse
    break
fig = plt.figure(figsize=(5,5))
plt.scatter(df_new['x'], df_new['y'], color=df_new['color']) # k means black
for i in new_centroids.keys():
  plt.scatter(*new_centroids[i], color=color_dic2[i]) # overlaying centroids
plt.xlim(0,80) # min and max of scale on x -axis
plt.ylim(0,80) # same as above on y axis
plt.show()

""