# -*- coding: utf-8 -*-
"""Classification_Test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o19xLf7BXqblSouB8zoAmhA770LaB49_
"""

# Complete the notebook to perform

# Data Exploration

# Feature Engineering

# Model Training and Evaluation to select the best model

import pandas as pd
URL = 'https://raw.githubusercontent.com/a-forty-two/EY8Apr2024-AI-Batch2/main/diabetes.csv'
data = pd.read_csv(URL, header=0)
data.head()

y = data['Outcome']
x = data.iloc[:, 1:]

from sklearn.model_selection import train_test_split
xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.20)

xtrain.describe()

mu = xtrain.describe().T['mean']
sigma = xtrain.describe().T['std']

#normalize the data for training and testing
xtrain_norm = (xtrain - mu) / sigma
xtest_norm = (xtest - mu) / sigma

xtrain_norm.head()

# outliers
outliers = (xtrain_norm > 3) | (xtrain_norm < -3)
xtrain_norm['isOutlier'] = outliers.sum(axis=1)
all_outliers = xtrain_norm[xtrain_norm['isOutlier'] > 0]
all_outliers.index

xtrain_norm_backup = xtrain_norm

all_outliers

for myindex in all_outliers.index:
  for col_name in xtrain_norm.columns:
    datapoint = xtrain_norm.loc[myindex, [col_name]]
    if(datapoint[0] > 3):
      xtrain_norm.loc[myindex, [col_name]] = 3.0

    if( datapoint[0] < -3 ):
      xtrain_norm.loc[myindex, [col_name]] = -3.0

# check again if any outliers are still present
outliers = (xtrain_norm > 3) | (xtrain_norm < -3)
xtrain_norm['isOutlier'] = outliers.sum(axis=1)
all_outliers = xtrain_norm[xtrain_norm['isOutlier'] > 0]
xtrain_norm = xtrain_norm.iloc[:, :-1]
print('Pending outliers = ')
print(all_outliers.index)
xtrain_norm.columns

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

# in case of trees-> we could fluctuate the depth of tree
# in case of ensembles (forests or jungles) -> we could fluctuate no. of trees

# Hyperparameters-> values that we adjust from our side as AI/ML engineers
# -> ASSUMPTIONS from our end
# -> these adjustments result in different models out of same algorithms!

my_algos = {
    "DecisionTree100": DecisionTreeClassifier(max_depth=100),
    "DecisionTree10": DecisionTreeClassifier(max_depth=10),
    "DecisionTree200": DecisionTreeClassifier(max_depth=200),
    "DecisionTree50": DecisionTreeClassifier(max_depth=50),
    "RandomForest10": RandomForestClassifier(n_estimators=10),
    "RandomForest100": RandomForestClassifier(n_estimators=100),
    "RandomForest200": RandomForestClassifier(n_estimators=200),
    "RandomForest300": RandomForestClassifier(n_estimators=300),
    "KNN" : KNeighborsClassifier(),
    "Sherlock" : GaussianNB(),
    "Support Vector Machine with linear" : SVC(kernel='linear'),
    "Support Vector Machines with RBF": SVC(kernel='rbf')
}

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
best_model_acc = None
best_model_f1 = None
best_model_p = None
best_model_r = None

best_model_name_acc = ""
best_model_name_f1 = ""
best_model_name_p = ""
best_model_name_r = ""

best_acc = -1
best_f1 = -1
best_precision = -1
best_recall = -1


all_accs = []
all_f1_scores = []
all_precions = []
all_recalls = []

for algo_name, algo in my_algos.items():
  print('Algorithm considered = ' + algo_name)
  model = algo
  model.fit(xtrain_norm, ytrain)
  p = model.predict(xtest_norm)
  a_score = accuracy_score(p, ytest)
  p_score = precision_score(p, ytest)
  r_score = recall_score(p, ytest)
  f_score = f1_score(p, ytest)


  all_accs.append(a_score)
  all_f1_scores.append(f_score)
  all_precions.append(p_score)
  all_recalls.append(r_score)
  print('Accuracy = ' + str(a_score))
  print('Precision = ' + str(p_score))
  print('Recall = ' + str(r_score))
  print('F1 Score = ' + str(f_score))


  #if a_score == 1.0: # if any model is 100% accurate, its a sign of overfitting
    #print('Overfitting')

  #else:
  if a_score > best_acc:
    best_acc = a_score
    best_model_acc = algo
    best_model_name_acc = algo_name
  if f_score > best_f1:
    best_f1 = f_score

    best_model_f1 = algo
    best_model_name_f1 = algo_name
  if p_score > best_precision:
    best_precision = p_score
    best_model_p = algo
    best_model_name_p = algo_name
  if r_score > best_recall:
    best_recall = r_score
    best_model_r = algo
    best_model_name_r = algo_name
  print('*************')
  print()

print(best_model_name_acc)
print(best_model_name_f1)
print(best_model_name_p)
print(best_model_name_r)

#So, DecisionTree100 is the best algorithm to fit the model.